# Fine-Tuning-Google-Gemma-2B-Using-LoRA-QLoRA
This project demonstrates how to fine-tune the Google Gemma 2B Instruct model using LoRA (Low-Rank Adaptation) and QLoRA (Quantized LoRA) to efficiently adapt a large language model for specialized tasks.  The goal was to explore Parameter Efficient Fine-Tuning (PEFT) techniques to:  Reduce training cost  Enable training on consumer-grade hardware 
